{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da91e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import numpy as np\n",
    "from loadimages import load_test_images\n",
    "import random\n",
    "from models import FlowNetVector, FlowNetS, FlowNetS4\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4d5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_s = load_test_images(1)\n",
    "sequences_s4 = load_test_images(3)\n",
    "sequences_v = load_test_images(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c4068e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903c9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "avg_pool = nn.AvgPool2d(2)\n",
    "upsample_16 = nn.Upsample(scale_factor=16, mode='nearest')\n",
    "upsample_32 = nn.Upsample(scale_factor=32, mode='nearest')\n",
    "upsample_64 = nn.Upsample(scale_factor=64, mode='nearest')\n",
    "upsample_128 = nn.Upsample(scale_factor=128, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e1b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_vector(block1, block2, block_size):\n",
    "  x_y = [0, 0]\n",
    "  min_error = float('inf')\n",
    "  for i in range(2 * block_size):\n",
    "    for j in range(2 * block_size):\n",
    "      error = mse_loss(block1, block2[:,i:i + block_size, j:j + block_size])\n",
    "      if error < min_error:\n",
    "        min_error = error\n",
    "        x_y = [block_size - i - 1, block_size - j - 1]\n",
    "  return x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b33e6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_layer(image1, image2, size):\n",
    "  vectors = np.zeros(size)\n",
    "  for j in range(1, size[0] + 1):\n",
    "    for k in range(1, size[1] + 1):\n",
    "      x_y = get_block_vector(\n",
    "        image1[:,j * block_size: j * block_size + block_size,k * block_size: k * block_size + block_size],\n",
    "        image2[:,(j - 1) * block_size: (j + 2) * block_size,(k - 1) * block_size: (k + 2) * block_size],\n",
    "        block_size\n",
    "      )\n",
    "      vectors[j - 1][k - 1][0] = x_y[0]\n",
    "      vectors[j - 1][k - 1][1] = x_y[1]\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e465d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsize_images(image1, image2):\n",
    "  image1 = avg_pool(image1)\n",
    "  image2 = avg_pool(image2)\n",
    "  return (image1, image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5843aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 16\n",
    "def find_motion_images(image1, image2):\n",
    "  first_layer_vector_image = get_vector_layer(image1, image2, (30, 22, 2))\n",
    "\n",
    "  (image1, image2) = downsize_images(image1, image2)\n",
    "  second_layer_vector_image = get_vector_layer(image1, image2, (14, 10, 2))\n",
    "\n",
    "  (image1, image2) = downsize_images(image1, image2)\n",
    "  third_layer_vector_image = get_vector_layer(image1, image2, (6, 4, 2))\n",
    "\n",
    "  (image1, image2) = downsize_images(image1, image2)\n",
    "  fourth_layer_vector_image = get_vector_layer(image1, image2, (2, 1, 2))\n",
    "  \n",
    "  img_f1_x = upsample_16(torch.tensor(np.pad(first_layer_vector_image[:,:,0], ((1, 1), (1, 1)), mode='constant')).unsqueeze(0).unsqueeze(0).type(torch.FloatTensor)).squeeze(0)\n",
    "  img_f1_y = upsample_16(torch.tensor(np.pad(first_layer_vector_image[:,:,1], ((1, 1), (1, 1)), mode='constant')).unsqueeze(0).unsqueeze(0).type(torch.FloatTensor)).squeeze(0)\n",
    "  img_f2_x = upsample_32(torch.tensor(np.pad(second_layer_vector_image[:,:,0], ((1, 1), (1, 1)), mode='constant')).unsqueeze(0).unsqueeze(0).type(torch.FloatTensor)).squeeze(0)\n",
    "  img_f2_y = upsample_32(torch.tensor(np.pad(second_layer_vector_image[:,:,1], ((1, 1), (1, 1)), mode='constant')).unsqueeze(0).unsqueeze(0).type(torch.FloatTensor)).squeeze(0)\n",
    "  img_f3_x = upsample_64(torch.tensor(np.pad(third_layer_vector_image[:,:,1], ((1, 1), (1, 1)), mode='constant')).unsqueeze(0).unsqueeze(0).type(torch.FloatTensor)).squeeze(0)\n",
    "  img_f3_y = upsample_64(torch.tensor(np.pad(third_layer_vector_image[:,:,1], ((1, 1), (1, 1)), mode='constant')).unsqueeze(0).unsqueeze(0).type(torch.FloatTensor)).squeeze(0)\n",
    "  img_f4_x = upsample_128(torch.tensor(np.pad(fourth_layer_vector_image[:,:,0], ((1, 1), (1, 1)), mode='constant')).unsqueeze(0).unsqueeze(0).type(torch.FloatTensor)).squeeze(0)\n",
    "  img_f4_y = upsample_128(torch.tensor(np.pad(fourth_layer_vector_image[:,:,1], ((1, 1), (1, 1)), mode='constant')).unsqueeze(0).unsqueeze(0).type(torch.FloatTensor)).squeeze(0)\n",
    "\n",
    "  return  (img_f1_x, img_f1_y, img_f2_x, img_f2_y, img_f3_x, img_f3_y, img_f4_x, img_f4_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f701ca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n",
      "torch.Size([1, 512, 384])\n"
     ]
    }
   ],
   "source": [
    "vectors = list()\n",
    "for sequence in sequences_v:\n",
    "  (img_f1_x, img_f1_y, img_f2_x, img_f2_y, img_f3_x, img_f3_y, img_f4_x, img_f4_y) = find_motion_images(sequence[0][:3], sequence[0][3:6])\n",
    "  print(img_f1_x.shape)\n",
    "  vectors.append([\n",
    "    torch.cat((\n",
    "      sequence[0], img_f1_x, img_f1_y, img_f2_x, img_f2_y, img_f3_x, img_f3_y, img_f4_x, img_f4_y\n",
    "    ), 0).type(torch.FloatTensor),\n",
    "    sequence[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13e85916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s = FlowNetS()\n",
    "model_s.load_state_dict(torch.load('./flownets.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "model_s4 = FlowNetS4()\n",
    "model_s4.load_state_dict(torch.load('./flownets4.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "model_v = FlowNetVector()\n",
    "model_v.load_state_dict(torch.load('./vectornet.pt', map_location=torch.device('cpu')))\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b1b64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sequences_s)): # change this to sequences\n",
    "  test_img = cv2.resize(sequences_s[i][1].cpu().permute(1, 2, 0).detach().numpy(), dsize=(192, 256), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "  out = model_v(vectors[i][0].unsqueeze(0)).squeeze(0).cpu()\n",
    "  vector_img = cv2.resize(out.permute(1, 2, 0).detach().numpy(), dsize=(192, 256), interpolation=cv2.INTER_CUBIC)\n",
    "  vector_difference = vector_img - test_img\n",
    "  \n",
    "  out = model_s(sequences_s[i][0].unsqueeze(0)).squeeze(0).cpu()\n",
    "  s_img = cv2.resize(out.permute(1, 2, 0).detach().numpy(), dsize=(192, 256), interpolation=cv2.INTER_CUBIC)\n",
    "  s_difference = s_img - test_img\n",
    "    \n",
    "  out = model_s4(sequences_s4[i][0].unsqueeze(0)).squeeze(0).cpu()\n",
    "  s4_img = cv2.resize(out.permute(1, 2, 0).detach().numpy(), dsize=(192, 256), interpolation=cv2.INTER_CUBIC)\n",
    "  s4_difference = s4_img - test_img\n",
    "\n",
    "  first = cv2.resize(sequences_s[i][0][:3].cpu().permute(1, 2, 0).detach().numpy(), dsize=(192, 256), interpolation=cv2.INTER_CUBIC)\n",
    "  second = cv2.resize(sequences_s[i][0][3:6].cpu().permute(1, 2, 0).detach().numpy(), dsize=(192, 256), interpolation=cv2.INTER_CUBIC)\n",
    "  \n",
    "  cv2.imwrite('before' + str(i) + '.jpg', first)\n",
    "  cv2.imwrite('after' + str(i) + '.jpg', second)\n",
    "    \n",
    "  cv2.imwrite('generated vector' + str(i) + '.jpg', vector_img)\n",
    "  cv2.imwrite('generated vector difference' + str(i) + '.jpg', vector_difference)\n",
    "    \n",
    "  cv2.imwrite('generated s' + str(i) + '.jpg', s_img)\n",
    "  cv2.imwrite('generated s difference' + str(i) + '.jpg', s_difference)\n",
    "\n",
    "  cv2.imwrite('generated s4' + str(i) + '.jpg', s4_img)\n",
    "  cv2.imwrite('generated s4 difference' + str(i) + '.jpg', s4_difference)\n",
    "    \n",
    "  cv2.imwrite('test' + str(i) + '.jpg', test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d476ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
